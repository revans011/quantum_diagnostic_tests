---
title: "Analysis"
date: "`r Sys.Date()`"
author: "Rich Evans, PhD, PSTAT"
format: pdf
number-sections: true
execute: 
  echo: false
output-file: "analysis_results"
---




```{r}
#| label: setup
#| echo: false
#| include: false

# a little template for installing. BUT perhaps better to use pacman for many packages
# if (!requireNamespace("bnlearn", quietly = TRUE)) {
#   install.packages("bnlearn", quietly = TRUE)}

# Set global chunk options
knitr::opts_chunk$set(
  echo = FALSE,         # Show code by default
  warning = FALSE,     # Hide warnings in the output
  message = FALSE,     # Hide messages in the output
  fig.width = 4,       # Default figure width
  fig.height = 2.666,      # Default figure height
  fig.align = "center" # Center align figures
)

# Set seed for reproducibility
set.seed(1234)

```

```{r}
#| label: read-data
#| include: false

#------------------------csv-----------------------------

df <- readr::read_csv(here("data","lobanstudy.csv"))

df <- clean_names(df)

#-----------------------------excel---------------------

df <- readxl::read_xlsx(here("data","re.xlsx"), sheet="cleaned", na = c("NF", "NA", "N/A"))
#df <- readr::read_csv(here("data","lobanstudy.csv"))


df <- clean_names(df)

skim(df)

#---------------------------multiple sheets----------------------------

df_list = here("data","Katie_Final_Data_Collection_Tables.xlsx") %>% 
  excel_sheets() %>% 
  purrr::set_names() %>% 
    map(~ read_excel(path = here("data","Katie_Final_Data_Collection_Tables.xlsx"), sheet = .x, na = c("N/A", "Missing")))


df_list <- lapply(df_list,clean_names)

remove_na_columns <- function(df) {
  df[, colSums(is.na(df)) == 0]
}

# Apply the function to each dataframe in the list to remove empty columns
df_list_cleaned <- lapply(df_list, remove_na_columns)


##### remove useless or empty columns ------------------------------------
#useless columns
df <- df %>%
  select(-starts_with("x")) # removes columns that have info but are not named

#removes empty columns
df <- df %>%
  select(where(~ !all(is.na(.x) | .x == "")))
##### remove useless columns -------------------------------
df <- df %>%
  select(-starts_with("x")) # removes columns that have info but are not named

#removes empty columns
df <- df %>%
  select(where(~ !all(is.na(.x) | .x == "")))


write.csv(df,here("data","df.csv"))
```



```{python}
#| label: AI
#| eval: FALSE


"""

You are a biostatistician. 


Write R code to do FOUR kaplan-Meier (KM) analyses on this dataset. 

STEP ONE (write R code to create two new variables)
  (a) for overall survival, calculate the time in months between date of diagnosis ("date_of_dx_d0_for_os") and date of death or censoring ("date_of_dx_d0_for_os"). Call that variable "time_os_months"

  (b) for progression free survival, calculate the time in months between end of radiation ("date_of_rt2_end_d0_for_pfs") and date of progression or censoring ("date_of_rec2_end_for_pfs"). Call that variable "time_pfs_months"


STEP TWO
Write R code to do:
  (a) a KM analysis for overall survival using  time_os_months and the event variable "alive_0_or_dead_1"
  (b) a KM analysis by group for overall survival using  time_os_months, the event variable "alive_0_or_dead_1", and the grouping variable "group_name_hfrt_or_crt". Include a graph and log rank test. Put the p-value of the log-rank test on the graph.
  (c) a KM analysis for overall pfs using  time_pfs_months and the event variable "recurrence_0_or_no_recurrence_1"
  (d) a KM analysis by group for pfs using  time_pfs_months, the event variable "recurrence_0_or_no_recurrence_1", and the grouping variable "group_name_hfrt_or_crt". Include a graph and log rank test. Put the p-value of the log-rank test on the graph.

STEP THREE
Write R code to 
  (a) do descriptive statistics on "toxicities_y_or_n", "worse_than_g1_tox_y_or_n",  "alopecia" , "fatigue", "dermatitis",  "ha", "other".
  (b) do non-parametric group testing using the grouping variable "group_name_hfrt_or_crt" for the variables "toxicities_y_or_n", "worse_than_g1_tox_y_or_n",  "alopecia" , "fatigue", "dermatitis",  "ha", "other".
  
STEP FOUR
Write the statistical methods section of a manuscript describing the statistical analysis that was done in STEP ONE, STEP TWO, AND STEP THREE.

"""
```



```{r}
#| label: fix-some-values
#| include: false


df <- df |> mutate(
  surgical_prep = ifelse(surgical_prep=="4% Chlorhexidine Scrub\n","4% Chlorhexidine Scrub",surgical_prep),
  surgery = ifelse(surgery=="TLPLO","TPLO",surgery)
)

```


```{r}
#| label: table-one
#| include: false

# OPTIONAL: list columns to include (otherwise it will use all)
# vars_for_summary <- c("x1", "x2", "x3", "group_var")


by_var <- "ioban_no_ioban"  # <- replace with your grouping variable name

tbl1 <-  df %>% select(-c(days, patient_id)) |>
  # If you defined vars_for_summary above, use:
  # select(all_of(vars_for_summary)) %>%
tbl_summary(
    by   = all_of(by_var),
    digits = all_continuous() ~ 3,
    type = list(
      all_continuous()  ~ "continuous2",  # enables two-lines stats per continuous var
      all_dichotomous() ~ "categorical"   # show BOTH levels for 0/1 variables
    ),
    statistic = list(
      all_continuous2() ~ c("{mean} ({sd})", "{median} ({min}/{max})"),
      all_categorical() ~ "{n} ({p}%)"
    ),
    missing_text = "Missing"
  ) %>%
  add_p(test = list(
    all_continuous()  ~ "kruskal.test",  # robust across distributions
    all_categorical() ~ "fisher.test"    # safe for small counts
  )) %>%
  add_stat_label(label = all_continuous2() ~ c("Mean (SD)", "Median (Min/Max)")) %>%
  bold_labels() %>%                                       # <-- bold variable labels
  modify_header(label = "**Descriptive Summary**")  # <-- title

#make a tibble for extracting elements of table one
df_tbl1 <- as_tibble(tbl1)

#this is a "pandas" type way of extracting elements of table one for `r foo` in results
# foo <- df_tbl1[df_tbl1$"**Descriptive Summary by ioban or no ioban**"=="__age__", "**p-value**"]


#get table one ready for rendering
tbl1 <- tbl1 |>  
  as_flex_table() %>%
  flextable::set_table_properties(width = 1, layout = "autofit") %>%
  flextable::fontsize(size = 8)

```

# Comments

Summary statistics are in @tbl-table-one-show. At this point the idea is to check the table and make sure the data seem as they should. Especially, check the variables' values for typos. As an example, for a hypothetical sex variable, the computer treats _Male_ and _male_ as different sexes.

# Statistical Methods

Continuous variables were summarized using both means with standard deviations and medians with minimum and maximum values to provide central tendency and data dispersion. Comparisons between groups were assessed using the non-parametric Kruskal–Wallis test for continuous variables, and Fisher’s exact test for categorical variables, to account for data asymmetry and small sample sizes.

# Notes

1. Interpret the p-values cautiously because some of the variables have a large number of missing values. Variables with a large number of missing values should be reported with summary statistics (e.g., medians) with p-values omitted.


\newpage

# Results


## Table One

```{r}
#| label: tbl-table-one-show

tbl1 
```


<!--  \vspace{5cm}  -->

\newpage

# Appendicies

## Software References

TBD

```{r}
#| label: references
#| eval: false

# Get citation for R without the extra message or BibTeX
r_citation <- citation()
print(r_citation, style = "text")  # Suppresses extra messages and BibTeX



# Utility to print version and non-BibTeX citation for a package
print_pkg_info <- function(pkg) {
  # Header line
  cat(sprintf("#------------ %s ---------\n", pkg))
  # Version
  ver <- as.character(utils::packageVersion(pkg))
  cat(pkg, "package version:", ver, "\n")
  # Citation (text style to suppress BibTeX)
  cit <- utils::citation(pkg)
  print(cit, style = "text")
  cat("\n\n")
}

# Packages requested
pkgs <- c("stringr", "brms", "survival", "posterior", "purrr", "bayesplot", "rstan","pwr", "tidyverse", "gtsummary", "stats", "ggplot2")

# Print info for each
invisible(lapply(pkgs, print_pkg_info))


```


## Boilerplate (Everyone gets these!)

1. The variable names are changed slightly to make programming easier. 

2. P =< 0.05 means that it is likely the populations have different parameters (e.g., population means, population medians). On the flip side, P > 0.05 means "no conclusion," that is, _there was not enough evidence to conclude the populations have different parameters_. P > 0.05 does **not** mean "no group differences." Moreover, the groups' data are almost always different. P-values are statements about population parameters, not the data, and p > 0.05 mean that we can not reach a conclusion about the population parameters. This comes from the fundamentals: Fisher, writing in 1935, said: "it should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation." (Fisher, 1990b, p. 16). (Fisher, R.A.: The Design of Experiments (reprinted 8th edition, 1966). In: Bennett, J.H. (ed.) Statistical Methods, Experimental Design, and Scientific Inference. Oxford University Press, Oxford (1990b))

3. Checking many p-values for statistical significance increases the chance of a false positive result. Checking six p-values makes the chance of a false positive result 26%. Checking 14 p-values increase the chance of a false positive to 51%. I can make corrections to bring those high error rates (e.g., 51%) back to  5% (=0.05). This problem is sometimes called multiplicity or Type I error inflation.

4. The tables are figures are **not** publication ready. Fine-tuning tables and figures takes a lot of time, so I save fine-tuning until the final ones are chosen for publication. That way, I'm not fine-tuning things that are discarded in the final draft.


**End of Document**

Short answer: γ is constrained correctly; κ is physically fine but you’re implicitly restricting it to be nonnegative. That’s OK if you don’t care about its sign (which you usually don’t), but it’s good to be explicit about what you mean.

Let me break it down parameter by parameter.

⸻

1. κ (state coherence)

From the density matrix constraint,

\rho =
\begin{pmatrix}
1-\pi & c \\
c^* & \pi
\end{pmatrix},
\qquad
c = \kappa \sqrt{\pi(1-\pi)},

positivity requires

|c|^2 \le \pi(1-\pi)
\quad\Longrightarrow\quad
|\kappa| \le 1.

So the only hard constraint is

\kappa \in [-1,1].

In the Stan/R code I wrote, I used:

real<lower=0,upper=1> kappa;
kappa ~ beta(a_kappa, b_kappa);

So:
	•	This respects the physical constraint (you’re in [0,1]\subset[-1,1]).
	•	It implicitly assumes nonnegative coherence amplitude.
	•	The sign of c can always be absorbed into a phase if you later decide to go fully complex; so restricting κ ≥ 0 is not a physical violation, just a modeling choice.

If you want full generality:
	•	Let \kappa \in [-1,1] and put, say, a truncated Normal or uniform on that interval, or
	•	Keep \kappa \in [0,1] as a magnitude and carry a separate phase parameter for the sign.

For the current real-valued, phase-stripped model, κ ∈ [0,1] with a Beta prior is perfectly valid and safely constrained.

⸻

2. γ (measurement coherence)

The POVM element

E_+ =
\begin{pmatrix}
1-\mathrm{Sp} & \gamma \\
\gamma        & \mathrm{Se}
\end{pmatrix}

must be positive semidefinite, which gives:

\gamma^2 \le (1-\mathrm{Sp})\,\mathrm{Se}.

So the allowed range for γ depends on Se and Sp.
If you just did \gamma \sim \text{Uniform}(-1,1), you’d often violate this.

In the Stan code, I enforced:

real<lower=-1,upper=1> eta;
gamma = eta * sqrt( (1 - Sp) * Se );

This implies

|\gamma| = |\eta| \sqrt{(1-\mathrm{Sp})\,\mathrm{Se}}
\le \sqrt{(1-\mathrm{Sp})\,\mathrm{Se}},

because |\eta|\le 1. So:
	•	The positivity constraint on E_+ is automatically satisfied.
	•	The prior on γ is induced via a prior on η:

eta ~ uniform(-1, 1);  // or truncated Normal around 0



This is exactly the correct way to constrain γ:
you put the prior on η \in[-1,1], not directly on γ.

⸻

3. Does this also guarantee 0 \le p_+ \le 1?

Yes. Under:
	•	\pi,\mathrm{Se},\mathrm{Sp} \in [0,1],
	•	|\kappa|\le 1,
	•	|\gamma| \le \sqrt{(1-\mathrm{Sp})\,\mathrm{Se}},

you’re guaranteed that:
	•	\rho is a valid density matrix,
	•	0 \preceq E_+ \preceq I,

and therefore

p_+ = \mathrm{Tr}(E_+\rho) \in [0,1].

So the constraints you’ve used do keep you in the physically/ probabilistically valid region.

⸻

4. If you want to be extra explicit in the writeup

You can safely say:
	•	κ:
“We restrict 0 \le \kappa \le 1, interpreting κ as a nonnegative coherence magnitude; the sign can be absorbed into an implicit phase.”
	•	γ:
“We parameterize \gamma = \eta\sqrt{(1-\mathrm{Sp})\,\mathrm{Se}} with \eta \in [-1,1], which guarantees that the POVM element E_+ remains positive semidefinite for all Se, Sp in (0,1).”

So: yes, the priors are constrained properly for physical validity; κ is additionally restricted to be nonnegative by design, which is a modeling choice rather than a violation.

```{r}
stan_code <- '
data {
  int<lower=0> N;          // number of subjects
  int<lower=0, upper=N> y; // number of positive tests

  // hyperparameters for Beta priors
  real<lower=0> a_pi;
  real<lower=0> b_pi;
  real<lower=0> a_Se;
  real<lower=0> b_Se;
  real<lower=0> a_Sp;
  real<lower=0> b_Sp;
  real<lower=0> a_kappa;
  real<lower=0> b_kappa;
}

parameters {
  real<lower=0,upper=1> pi;     // prevalence
  real<lower=0,upper=1> Se;     // sensitivity
  real<lower=0,upper=1> Sp;     // specificity
  real<lower=0,upper=1> kappa;  // state coherence
  real<lower=-1,upper=1> eta;   // scaled measurement coherence
}

transformed parameters {
  real gamma;  // measurement coherence parameter
  real p_pos;  // P(T = 1)

  // enforce |gamma| <= sqrt((1-Sp)*Se)
  gamma = eta * sqrt( (1 - Sp) * Se );

  // quantum-inspired probability of positive test
  p_pos = (1 - Sp) * (1 - pi)
          + Se * pi
          + 2 * gamma * kappa * sqrt( pi * (1 - pi) );
}

model {
  // priors
  pi    ~ beta(a_pi,    b_pi);
  Se    ~ beta(a_Se,    b_Se);
  Sp    ~ beta(a_Sp,    b_Sp);
  kappa ~ beta(a_kappa, b_kappa);
  eta   ~ uniform(-1, 1);   // shrinkage priors could be used instead

  // likelihood
  y ~ binomial(N, p_pos);
}
'

```


```{r}
writeLines(stan_code, con = "quantum_one_test.stan")
```


```{r}
# install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)

# make sure cmdstan is installed once:
# cmdstanr::install_cmdstan()

# Compile the Stan model
mod <- cmdstan_model("quantum_one_test.stan")

# Example data
N <- 200
y <- 47

# Prior hyperparameters (adjust to taste)
stan_data <- list(
  N       = N,
  y       = y,
  a_pi    = 1, b_pi    = 1,   # flat prior on pi
  a_Se    = 2, b_Se    = 2,   # mild prior on Se
  a_Sp    = 2, b_Sp    = 2,   # mild prior on Sp
  a_kappa = 1, b_kappa = 1    # flat prior on kappa
)

# Run HMC NUTS
fit <- mod$sample(
  data       = stan_data,
  seed       = 1234,
  chains     = 4,
  parallel_chains = 4,
  iter_warmup = 2000,
  iter_sampling = 2000,
  adapt_delta = 0.9   # NUTS tuning parameter
)

# Print summary for key parameters
fit$summary(variables = c("pi", "Se", "Sp", "kappa", "gamma"))

```

